{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Set absolute path\n",
        "nb_path = os.path.abspath(\"a2.ipynb\")\n",
        "# nb_path = \"/content/drive/MyDrive/Colab Notebooks/cisc351-a2-fashion-classifier/images/a2.ipynb\"  # For Google Colab\n",
        "DIR_PATH = os.path.dirname(nb_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>label</th>\n",
              "      <th>productname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2653</td>\n",
              "      <td>Bags</td>\n",
              "      <td>Murcia Women Leather Office Grey Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55997</td>\n",
              "      <td>Others</td>\n",
              "      <td>Colorbar Velvet Matte Temptation Lipstick 24MA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2640</td>\n",
              "      <td>Shoes</td>\n",
              "      <td>Carlton London Men Brown Formal Shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40565</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>W Women Maroon Kurta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38932</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Gini and Jony Girls Pink Leggings</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid       label                                     productname\n",
              "0     2653        Bags            Murcia Women Leather Office Grey Bag\n",
              "1    55997      Others  Colorbar Velvet Matte Temptation Lipstick 24MA\n",
              "2     2640       Shoes           Carlton London Men Brown Formal Shoes\n",
              "3    40565     Topwear                            W Women Maroon Kurta\n",
              "4    38932  Bottomwear               Gini and Jony Girls Pink Leggings"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load and preview the dataset\n",
        "train_df = pd.read_csv(os.path.join(DIR_PATH, 'train.csv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(DIR_PATH, 'test.csv'), sep='\\t')\n",
        "\n",
        "df = pd.concat([train_df, test_df], axis=0)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* FREQUENCY BY CATEGORY *\n",
            "label\n",
            "Topwear       15401\n",
            "Shoes          7344\n",
            "Others         6230\n",
            "Bags           3055\n",
            "Bottomwear     2693\n",
            "Watches        2542\n",
            "Innerwear      1808\n",
            "Jewellery      1080\n",
            "Eyewear        1073\n",
            "Fragrance      1012\n",
            "Sandal          963\n",
            "Wallets         933\n",
            "Makeup          307\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Make sure these are the only categories that appear in the dataset\n",
        "labels = set({\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"})\n",
        "unique_labels = df[\"label\"].unique()\n",
        "assert(labels == set(unique_labels))\n",
        "\n",
        "print(\"* FREQUENCY BY CATEGORY *\")\n",
        "print(df[\"label\"].value_counts(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS device.\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device.\")\n",
        "# Check if Mac GPU acceleration is available\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"]\n",
        "label_dict = {label: index for index, label in enumerate(class_labels)}\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_file, images_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): path to csv file with `imageid` (file name) and `label`.\n",
        "            images_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(os.path.join(DIR_PATH, csv_file), sep='\\t')\n",
        "        self.df[\"label\"] = self.df[\"label\"].apply(lambda x: label_dict[x])  # convert the labels to numbers\n",
        "        self.images_dir = os.path.join(DIR_PATH, images_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = f\"{self.df.iloc[idx, 0]}.jpg\"\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert('RGB')  # some images are in grayscale\n",
        "        label = self.df.iloc[idx, 1]  # label is the second column\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_mean_std(loader: DataLoader):\n",
        "    \"\"\"\n",
        "    Compute the mean and standard deviation of the dataset for normalization.\n",
        "    Args:\n",
        "        loader (DataLoader): DataLoader with images to compute the mean and std of.\n",
        "    \"\"\"\n",
        "    mean = 0\n",
        "    std = 0\n",
        "    img_count = 0\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        img_count += images.size(0)\n",
        "    mean /= img_count\n",
        "    std /= img_count\n",
        "    return mean, std\n",
        "\n",
        "# Define the transformations for the initial loader to compute the mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create a loader for computing the mean and std of the dataset, which we will use for normalization\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "mean, std = get_mean_std(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 1 - Baseline CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Define the transformations for the actual train and test loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
        "test_data = FashionDataset(\"test.csv\", \"images\", transform=transform)\n",
        "\n",
        "validation_data, test_data = train_test_split(test_data, test_size=0.5)  # split the test data into validation and test sets\n",
        "\n",
        "# Create the loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build CNN #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a CNN model to classify the images\n",
        "class FashionClassifierCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionClassifierCNN, self).__init__()\n",
        "        # [(input - filter + 2*pad) / stride] + 1\n",
        "        # 72x72x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
        "        # 68x68x16\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 34x34x16\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        # 30x30x32\n",
        "        # apply max pooling again\n",
        "        # 15x15x32\n",
        "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 13)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = FashionClassifierCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Rationale for my CNN architecture\n",
        "My CNN has two convolutional layers, each followed by a max pooling layer, and then three fully connected layers. This is a very standard architecture for image classification. The convolutional layers capture the low-level features of the images, such as edges and corners, and the last three fully-connected layers are capture the high-level features, such as the shape of the clothing item.\n",
        "\n",
        "- I chose the ReLU activation function because it is simple and efficient for introducing non-linearity into the model.\n",
        "- I chose the cross-entropy loss function (which applies a softmax activation function automatically) for the output layer because it is a standard choice for multi-class classification problems.\n",
        "- I chose the Adam optimizer with the standard learning rate of 0.001 because it adjusts the learning rate during training to improve training speed and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train CNN #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1, time remaining: 53.5 min]   train loss: 0.500   validation loss: 0.270\n",
            "[epoch 2, time remaining: 51.7 min]   train loss: 0.235   validation loss: 0.249\n",
            "[epoch 3, time remaining: 51.0 min]   train loss: 0.178   validation loss: 0.189\n",
            "[epoch 4, time remaining: 50.2 min]   train loss: 0.140   validation loss: 0.200\n",
            "[epoch 5, time remaining: 49.2 min]   train loss: 0.107   validation loss: 0.211\n",
            "[epoch 6, time remaining: 48.2 min]   train loss: 0.083   validation loss: 0.205\n",
            "[epoch 7, time remaining: 47.6 min]   train loss: 0.066   validation loss: 0.230\n",
            "[epoch 8, time remaining: 47.2 min]   train loss: 0.055   validation loss: 0.240\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 0.189) saved to model.pt\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model1.pt\")))\n",
        "\n",
        "epochs = 100\n",
        "no_improvement_streak = 0\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # For each batch in the training set\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()  # Reset the gradients\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # After each epoch, evaluate the model on the validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        validation_loss = 0.0\n",
        "        for i, data in enumerate(validation_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    time_elapsed = time.time() - time_start\n",
        "    time_left = (epochs - epoch - 1) / (epoch + 1) * time_elapsed\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_validation_loss = validation_loss / len(validation_loader)\n",
        "    \n",
        "    print(f\"[epoch {epoch+1}, time remaining: {time_left / 60:.1f} min]   train loss: {avg_train_loss:.3f}   validation loss: {avg_validation_loss:.3f}\")\n",
        "\n",
        "    # Early stopping mechanism based on validation loss\n",
        "    if avg_validation_loss < best_loss:\n",
        "        # save the model if it's the best so far\n",
        "        torch.save(model.state_dict(), os.path.join(DIR_PATH, \"model1.pt\"))\n",
        "        best_loss = avg_validation_loss\n",
        "        no_improvement_streak = 0\n",
        "    else:\n",
        "        no_improvement_streak += 1\n",
        "\n",
        "    if no_improvement_streak == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "\n",
        "print(f\"Finished training. Best model (validation loss: {best_loss:.3f}) saved to model1.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test CNN #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 2000 test images: 95.6%\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model = FashionClassifierCNN().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model1.pt\")))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # For each batch\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusion on the performance of the baseline model\n",
        "The baseline model performed very well, correctly classifying 95.6% of test samples. I used early stopping to halt training when there was no improvement in the validation loss for 5 epochs, and according to this rule training stopped after just 8 epochs -- this was potentially concerning. The training loss continued to decrease steadily while the validation loss plateaued, suggesting that the model quickly began overfitting the training data. I will address this in the next model through data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 2 - CNN with Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Define the transformations for randomized data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),  # Slight rotation, but not too much because clothing images are usually upright\n",
        "    transforms.RandomHorizontalFlip(),  # Do not use vertical flip for the same reason as above\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Adjust for different lighting conditions\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=train_transform)\n",
        "test_data = FashionDataset(\"test.csv\", \"images\", transform=test_transform)\n",
        "\n",
        "validation_data, test_data = train_test_split(test_data, test_size=0.5)  # split the test data into validation and test sets\n",
        "\n",
        "# Create the loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build CNN #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a CNN model to classify the images\n",
        "class FashionClassifierCNN3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionClassifierCNN3, self).__init__()\n",
        "        # [(input - filter + 2*pad) / stride] + 1\n",
        "        # 72x72x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
        "        # 68x68x16\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 34x34x16\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        # 30x30x32\n",
        "        # apply max pooling again\n",
        "        # 15x15x32\n",
        "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 13)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = FashionClassifierCNN3().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train CNN #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1, time remaining: 71.6 min]   train loss: 0.594   validation loss: 0.306\n",
            "[epoch 2, time remaining: 69.9 min]   train loss: 0.320   validation loss: 0.238\n",
            "[epoch 3, time remaining: 69.4 min]   train loss: 0.258   validation loss: 0.224\n",
            "[epoch 4, time remaining: 68.7 min]   train loss: 0.225   validation loss: 0.222\n",
            "[epoch 5, time remaining: 67.8 min]   train loss: 0.199   validation loss: 0.188\n",
            "[epoch 6, time remaining: 67.0 min]   train loss: 0.180   validation loss: 0.180\n",
            "[epoch 7, time remaining: 66.5 min]   train loss: 0.163   validation loss: 0.169\n",
            "[epoch 8, time remaining: 66.6 min]   train loss: 0.152   validation loss: 0.176\n",
            "[epoch 9, time remaining: 67.3 min]   train loss: 0.140   validation loss: 0.163\n",
            "[epoch 10, time remaining: 66.6 min]   train loss: 0.130   validation loss: 0.170\n",
            "[epoch 11, time remaining: 66.1 min]   train loss: 0.123   validation loss: 0.173\n",
            "[epoch 12, time remaining: 65.7 min]   train loss: 0.116   validation loss: 0.175\n",
            "[epoch 13, time remaining: 64.7 min]   train loss: 0.108   validation loss: 0.172\n",
            "[epoch 14, time remaining: 63.7 min]   train loss: 0.106   validation loss: 0.156\n",
            "[epoch 15, time remaining: 62.8 min]   train loss: 0.100   validation loss: 0.156\n",
            "[epoch 16, time remaining: 62.1 min]   train loss: 0.092   validation loss: 0.169\n",
            "[epoch 17, time remaining: 61.4 min]   train loss: 0.085   validation loss: 0.152\n",
            "[epoch 18, time remaining: 60.7 min]   train loss: 0.083   validation loss: 0.162\n",
            "[epoch 19, time remaining: 59.9 min]   train loss: 0.081   validation loss: 0.151\n",
            "[epoch 20, time remaining: 59.2 min]   train loss: 0.081   validation loss: 0.188\n",
            "[epoch 21, time remaining: 58.5 min]   train loss: 0.077   validation loss: 0.186\n",
            "[epoch 22, time remaining: 57.7 min]   train loss: 0.074   validation loss: 0.185\n",
            "[epoch 23, time remaining: 56.9 min]   train loss: 0.071   validation loss: 0.193\n",
            "[epoch 24, time remaining: 56.3 min]   train loss: 0.070   validation loss: 0.215\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 0.151) saved to model.pt\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model2.pt\")))\n",
        "\n",
        "epochs = 100\n",
        "no_improvement_streak = 0\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # For each batch in the training set\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()  # Reset the gradients\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # After each epoch, evaluate the model on the validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        validation_loss = 0.0\n",
        "        for i, data in enumerate(validation_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    time_elapsed = time.time() - time_start\n",
        "    time_left = (epochs - epoch - 1) / (epoch + 1) * time_elapsed\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_validation_loss = validation_loss / len(validation_loader)\n",
        "    \n",
        "    print(f\"[epoch {epoch+1}, time remaining: {time_left / 60:.1f} min]   train loss: {avg_train_loss:.3f}   validation loss: {avg_validation_loss:.3f}\")\n",
        "\n",
        "    # Early stopping mechanism based on validation loss\n",
        "    if avg_validation_loss < best_loss:\n",
        "        # save the model if it's the best so far\n",
        "        torch.save(model.state_dict(), os.path.join(DIR_PATH, \"model2.pt\"))\n",
        "        best_loss = avg_validation_loss\n",
        "        no_improvement_streak = 0\n",
        "    else:\n",
        "        no_improvement_streak += 1\n",
        "\n",
        "    if no_improvement_streak == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "\n",
        "print(f\"Finished training. Best model (validation loss: {best_loss:.3f}) saved to model2.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test CNN #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 2000 test images: 94.75%\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model = FashionClassifierCNN().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model2.pt\")))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # For each batch\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusion on the performance of the model with data augmentation\n",
        "The model with data augmentation performed slightly worse on test data than the first model, correctly classifying 94.75% of test samples. However, the training and validation losses decreased together throughout training, and the model did not begin overfitting the training data until after more than 15 epochs. This is a significant improvement over the baseline model, which began overfitting after only around 3 epochs.\n",
        "\n",
        "The training loss stayed higher for longer in this model because the images were randomly transformed each epoch, so the model did not see the exact same image twice. The fact that the ratio between the training and validation loss was closer to 1 for much of the training process means the model with data augmentation may generalize better to new data.\n",
        "\n",
        "So, while the model with data augmentation did not perform better than the baseline model on the test data, it is likely a better model overall because its lower validation loss and higher ratio between training and validation loss indicates it is less prone to overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 3 - Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
        "test_data = FashionDataset(\"test.csv\", \"images\", transform=transform)\n",
        "\n",
        "validation_data, test_data = train_test_split(test_data, test_size=0.5)  # split the test data into validation and test sets\n",
        "\n",
        "# Create the loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build CNN #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a CNN model to classify the images\n",
        "class FashionClassifierCNN3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionClassifierCNN3, self).__init__()\n",
        "        # [(input - filter + 2*pad) / stride] + 1\n",
        "        # 72x72x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
        "        # 68x68x16\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 34x34x16\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        # 30x30x32\n",
        "        # apply max pooling again\n",
        "        # 15x15x32\n",
        "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 13)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train CNN #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with learning rate 0.1\n",
            "[epoch 1, time remaining: 9.6 min]   train loss: 314.035   validation loss: 2.084\n",
            "[epoch 2, time remaining: 8.7 min]   train loss: 2.058   validation loss: 2.077\n",
            "[epoch 3, time remaining: 8.2 min]   train loss: 2.058   validation loss: 2.086\n",
            "[epoch 4, time remaining: 7.7 min]   train loss: 2.059   validation loss: 2.080\n",
            "[epoch 5, time remaining: 7.2 min]   train loss: 2.059   validation loss: 2.080\n",
            "[epoch 6, time remaining: 6.8 min]   train loss: 2.059   validation loss: 2.080\n",
            "[epoch 7, time remaining: 6.3 min]   train loss: 2.058   validation loss: 2.079\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 2.077) saved to model3_0.1.pt\n",
            "\n",
            "\n",
            "Training model with learning rate 0.01\n",
            "[epoch 1, time remaining: 9.0 min]   train loss: 2.124   validation loss: 2.083\n",
            "[epoch 2, time remaining: 8.6 min]   train loss: 2.056   validation loss: 2.076\n",
            "[epoch 3, time remaining: 8.1 min]   train loss: 2.056   validation loss: 2.078\n",
            "[epoch 4, time remaining: 7.6 min]   train loss: 2.055   validation loss: 2.076\n",
            "[epoch 5, time remaining: 7.2 min]   train loss: 2.055   validation loss: 2.078\n",
            "[epoch 6, time remaining: 6.7 min]   train loss: 2.055   validation loss: 2.077\n",
            "[epoch 7, time remaining: 6.3 min]   train loss: 2.054   validation loss: 2.077\n",
            "[epoch 8, time remaining: 5.8 min]   train loss: 2.054   validation loss: 2.077\n",
            "[epoch 9, time remaining: 5.3 min]   train loss: 2.054   validation loss: 2.077\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 2.076) saved to model3_0.01.pt\n",
            "\n",
            "\n",
            "Training model with learning rate 0.001\n",
            "[epoch 1, time remaining: 9.2 min]   train loss: 0.482   validation loss: 0.299\n",
            "[epoch 2, time remaining: 8.7 min]   train loss: 0.231   validation loss: 0.236\n",
            "[epoch 3, time remaining: 8.2 min]   train loss: 0.172   validation loss: 0.199\n",
            "[epoch 4, time remaining: 7.7 min]   train loss: 0.128   validation loss: 0.207\n",
            "[epoch 5, time remaining: 7.2 min]   train loss: 0.101   validation loss: 0.221\n",
            "[epoch 6, time remaining: 6.8 min]   train loss: 0.079   validation loss: 0.193\n",
            "[epoch 7, time remaining: 6.2 min]   train loss: 0.056   validation loss: 0.243\n",
            "[epoch 8, time remaining: 5.8 min]   train loss: 0.046   validation loss: 0.243\n",
            "[epoch 9, time remaining: 5.4 min]   train loss: 0.043   validation loss: 0.285\n",
            "[epoch 10, time remaining: 4.9 min]   train loss: 0.043   validation loss: 0.287\n",
            "[epoch 11, time remaining: 4.4 min]   train loss: 0.033   validation loss: 0.271\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 0.193) saved to model3_0.001.pt\n",
            "\n",
            "\n",
            "Training model with learning rate 0.0001\n",
            "[epoch 1, time remaining: 9.3 min]   train loss: 0.816   validation loss: 0.522\n",
            "[epoch 2, time remaining: 8.9 min]   train loss: 0.428   validation loss: 0.411\n",
            "[epoch 3, time remaining: 8.2 min]   train loss: 0.351   validation loss: 0.412\n",
            "[epoch 4, time remaining: 7.6 min]   train loss: 0.307   validation loss: 0.315\n",
            "[epoch 5, time remaining: 7.1 min]   train loss: 0.278   validation loss: 0.292\n",
            "[epoch 6, time remaining: 6.6 min]   train loss: 0.252   validation loss: 0.264\n",
            "[epoch 7, time remaining: 6.1 min]   train loss: 0.232   validation loss: 0.255\n",
            "[epoch 8, time remaining: 5.6 min]   train loss: 0.213   validation loss: 0.275\n",
            "[epoch 9, time remaining: 5.2 min]   train loss: 0.202   validation loss: 0.230\n",
            "[epoch 10, time remaining: 4.7 min]   train loss: 0.185   validation loss: 0.232\n",
            "[epoch 11, time remaining: 4.2 min]   train loss: 0.173   validation loss: 0.222\n",
            "[epoch 12, time remaining: 3.7 min]   train loss: 0.161   validation loss: 0.222\n",
            "[epoch 13, time remaining: 3.2 min]   train loss: 0.149   validation loss: 0.226\n",
            "[epoch 14, time remaining: 2.7 min]   train loss: 0.141   validation loss: 0.210\n",
            "[epoch 15, time remaining: 2.3 min]   train loss: 0.133   validation loss: 0.204\n",
            "[epoch 16, time remaining: 1.8 min]   train loss: 0.124   validation loss: 0.197\n",
            "[epoch 17, time remaining: 1.4 min]   train loss: 0.116   validation loss: 0.211\n",
            "[epoch 18, time remaining: 0.9 min]   train loss: 0.109   validation loss: 0.230\n",
            "[epoch 19, time remaining: 0.5 min]   train loss: 0.101   validation loss: 0.201\n",
            "[epoch 20, time remaining: 0.0 min]   train loss: 0.094   validation loss: 0.209\n",
            "Finished training. Best model (validation loss: 0.197) saved to model3_0.0001.pt\n",
            "\n",
            "\n",
            "Training model with learning rate 1e-05\n",
            "[epoch 1, time remaining: 8.6 min]   train loss: 1.874   validation loss: 1.329\n",
            "[epoch 2, time remaining: 8.0 min]   train loss: 1.026   validation loss: 0.876\n",
            "[epoch 3, time remaining: 7.5 min]   train loss: 0.759   validation loss: 0.736\n",
            "[epoch 4, time remaining: 7.0 min]   train loss: 0.654   validation loss: 0.657\n",
            "[epoch 5, time remaining: 6.6 min]   train loss: 0.588   validation loss: 0.607\n",
            "[epoch 6, time remaining: 6.1 min]   train loss: 0.540   validation loss: 0.569\n",
            "[epoch 7, time remaining: 5.7 min]   train loss: 0.503   validation loss: 0.531\n",
            "[epoch 8, time remaining: 5.3 min]   train loss: 0.473   validation loss: 0.520\n",
            "[epoch 9, time remaining: 4.9 min]   train loss: 0.449   validation loss: 0.488\n",
            "[epoch 10, time remaining: 4.5 min]   train loss: 0.429   validation loss: 0.472\n",
            "[epoch 11, time remaining: 4.0 min]   train loss: 0.413   validation loss: 0.449\n",
            "[epoch 12, time remaining: 3.6 min]   train loss: 0.397   validation loss: 0.436\n",
            "[epoch 13, time remaining: 3.1 min]   train loss: 0.385   validation loss: 0.422\n",
            "[epoch 14, time remaining: 2.7 min]   train loss: 0.374   validation loss: 0.411\n",
            "[epoch 15, time remaining: 2.2 min]   train loss: 0.363   validation loss: 0.402\n",
            "[epoch 16, time remaining: 1.8 min]   train loss: 0.355   validation loss: 0.388\n",
            "[epoch 17, time remaining: 1.3 min]   train loss: 0.347   validation loss: 0.381\n",
            "[epoch 18, time remaining: 0.9 min]   train loss: 0.338   validation loss: 0.373\n",
            "[epoch 19, time remaining: 0.4 min]   train loss: 0.331   validation loss: 0.373\n",
            "[epoch 20, time remaining: 0.0 min]   train loss: 0.325   validation loss: 0.359\n",
            "Finished training. Best model (validation loss: 0.359) saved to model3_1e-05.pt\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model3.pt\")))\n",
        "def train_model3(model):\n",
        "    epochs = 20\n",
        "    no_improvement_streak = 0\n",
        "    patience = 5\n",
        "    best_loss = float('inf')\n",
        "    time_start = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # For each batch in the training set\n",
        "        for i, data in enumerate(train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()  # Reset the gradients\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # After each epoch, evaluate the model on the validation set\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            validation_loss = 0.0\n",
        "            for i, data in enumerate(validation_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                validation_loss += loss.item()\n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        time_left = (epochs - epoch - 1) / (epoch + 1) * time_elapsed\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_validation_loss = validation_loss / len(validation_loader)\n",
        "        \n",
        "        print(f\"[epoch {epoch+1}, time remaining: {time_left / 60:.1f} min]   train loss: {avg_train_loss:.3f}   validation loss: {avg_validation_loss:.3f}\")\n",
        "\n",
        "        # Early stopping mechanism based on validation loss\n",
        "        if avg_validation_loss < best_loss:\n",
        "            # save the model if it's the best so far\n",
        "            torch.save(model.state_dict(), os.path.join(DIR_PATH, f\"model3_{lr}.pt\"))\n",
        "            best_loss = avg_validation_loss\n",
        "            no_improvement_streak = 0\n",
        "        else:\n",
        "            no_improvement_streak += 1\n",
        "\n",
        "        if no_improvement_streak == patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Finished training. Best model (validation loss: {best_loss:.3f}) saved to model3_{lr}.pt\")\n",
        "\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training model with learning rate {lr}\")\n",
        "    model = FashionClassifierCNN3().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    train_model3(model)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's keep training the model with lr=1e-5 because it was still improving when training ended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1, time remaining: 8.6 min]   train loss: 0.319   validation loss: 0.349\n",
            "[epoch 2, time remaining: 7.9 min]   train loss: 0.313   validation loss: 0.345\n",
            "[epoch 3, time remaining: 7.4 min]   train loss: 0.308   validation loss: 0.339\n",
            "[epoch 4, time remaining: 6.9 min]   train loss: 0.303   validation loss: 0.333\n",
            "[epoch 5, time remaining: 6.6 min]   train loss: 0.298   validation loss: 0.328\n",
            "[epoch 6, time remaining: 6.2 min]   train loss: 0.293   validation loss: 0.326\n",
            "[epoch 7, time remaining: 5.8 min]   train loss: 0.289   validation loss: 0.317\n",
            "[epoch 8, time remaining: 5.4 min]   train loss: 0.284   validation loss: 0.314\n",
            "[epoch 9, time remaining: 4.9 min]   train loss: 0.281   validation loss: 0.316\n",
            "[epoch 10, time remaining: 4.5 min]   train loss: 0.276   validation loss: 0.308\n",
            "[epoch 11, time remaining: 4.1 min]   train loss: 0.273   validation loss: 0.304\n",
            "[epoch 12, time remaining: 3.7 min]   train loss: 0.270   validation loss: 0.298\n",
            "[epoch 13, time remaining: 3.3 min]   train loss: 0.266   validation loss: 0.304\n",
            "[epoch 14, time remaining: 2.8 min]   train loss: 0.263   validation loss: 0.301\n",
            "[epoch 15, time remaining: 2.3 min]   train loss: 0.260   validation loss: 0.299\n",
            "[epoch 16, time remaining: 1.9 min]   train loss: 0.257   validation loss: 0.290\n",
            "[epoch 17, time remaining: 1.4 min]   train loss: 0.253   validation loss: 0.293\n",
            "[epoch 18, time remaining: 0.9 min]   train loss: 0.250   validation loss: 0.284\n",
            "[epoch 19, time remaining: 0.5 min]   train loss: 0.247   validation loss: 0.289\n",
            "[epoch 20, time remaining: 0.0 min]   train loss: 0.244   validation loss: 0.281\n",
            "Finished training. Best model (validation loss: 0.281) saved to model3_1e-05.pt\n"
          ]
        }
      ],
      "source": [
        "# load model from model3_1e-05.pt\n",
        "model = FashionClassifierCNN3().to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model3_1e-05.pt\")))\n",
        "\n",
        "# Set all parameters to require gradients so that we can continue training\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "train_model3(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1, time remaining: 8.8 min]   train loss: 0.242   validation loss: 0.278\n",
            "[epoch 2, time remaining: 8.3 min]   train loss: 0.239   validation loss: 0.275\n",
            "[epoch 3, time remaining: 7.8 min]   train loss: 0.237   validation loss: 0.286\n",
            "[epoch 4, time remaining: 7.4 min]   train loss: 0.234   validation loss: 0.273\n",
            "[epoch 5, time remaining: 6.9 min]   train loss: 0.232   validation loss: 0.269\n",
            "[epoch 6, time remaining: 6.4 min]   train loss: 0.229   validation loss: 0.272\n",
            "[epoch 7, time remaining: 6.0 min]   train loss: 0.227   validation loss: 0.272\n",
            "[epoch 8, time remaining: 5.5 min]   train loss: 0.224   validation loss: 0.262\n",
            "[epoch 9, time remaining: 5.0 min]   train loss: 0.223   validation loss: 0.267\n",
            "[epoch 10, time remaining: 4.6 min]   train loss: 0.220   validation loss: 0.260\n",
            "[epoch 11, time remaining: 4.1 min]   train loss: 0.218   validation loss: 0.263\n",
            "[epoch 12, time remaining: 3.7 min]   train loss: 0.218   validation loss: 0.252\n",
            "[epoch 13, time remaining: 3.2 min]   train loss: 0.215   validation loss: 0.258\n",
            "[epoch 14, time remaining: 2.8 min]   train loss: 0.212   validation loss: 0.258\n",
            "[epoch 15, time remaining: 2.3 min]   train loss: 0.210   validation loss: 0.259\n",
            "[epoch 16, time remaining: 1.8 min]   train loss: 0.208   validation loss: 0.259\n",
            "[epoch 17, time remaining: 1.4 min]   train loss: 0.206   validation loss: 0.256\n",
            "Early stopping!\n",
            "Finished training. Best model (validation loss: 0.252) saved to model3_1e-05.pt\n"
          ]
        }
      ],
      "source": [
        "train_model3(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test CNN #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(lr=0.1) Accuracy of the network on the 2000 test images: 35.0%\n",
            "(lr=0.01) Accuracy of the network on the 2000 test images: 35.0%\n",
            "(lr=0.001) Accuracy of the network on the 2000 test images: 94.95%\n",
            "(lr=0.0001) Accuracy of the network on the 2000 test images: 94.6%\n",
            "(lr=1e-05) Accuracy of the network on the 2000 test images: 92.65%\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model = FashionClassifierCNN().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model3.pt\")))\n",
        "for lr in learning_rates:\n",
        "    model = FashionClassifierCNN3().to(device)\n",
        "    model.load_state_dict(torch.load(os.path.join(DIR_PATH, f\"model3_{lr}.pt\")))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # For each batch\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"(lr={lr}) Accuracy of the network on the {total} test images: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion on the performance of the model with hyperparameter tuning\n",
        "I chose 5 learning rates separated by orders of magnitude, and the model with the learning rate of 0.001 performed the best. 0.001 is accepted as the default learning rate for Adam. The model with this learning rate correctly classified 94.95% of test samples, which is nearly the same as the model with data augmentation.\n",
        "\n",
        "The models with large learning rates 0.1 and 0.01 performed horribly with accuracies of 35% each. This is because the learning rate was too high, and the model was not able to converge to a good solution -- the training and validation losses hardly changed over the training process, exhibiting the model's frustration.\n",
        "\n",
        "The models with small learning rates 0.0001 and 0.00001 performed better than the models with large learning rates, but slightly worse than the model with the default learning rate. The two models, especially the 1e-5 model, also took significantly longer to train than the other models, marking the point where the precision that comes with a smaller learning rate is not worth the time it takes to train.\n",
        "\n",
        "In the end, it is not surprising that the model with the default learning rate performed the best because it balances speed and accuracy. 0.001 was likely chosen after testing extensively on diverse datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
