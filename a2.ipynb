{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>label</th>\n",
              "      <th>productname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2653</td>\n",
              "      <td>Bags</td>\n",
              "      <td>Murcia Women Leather Office Grey Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55997</td>\n",
              "      <td>Others</td>\n",
              "      <td>Colorbar Velvet Matte Temptation Lipstick 24MA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2640</td>\n",
              "      <td>Shoes</td>\n",
              "      <td>Carlton London Men Brown Formal Shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40565</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>W Women Maroon Kurta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38932</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Gini and Jony Girls Pink Leggings</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid       label                                     productname\n",
              "0     2653        Bags            Murcia Women Leather Office Grey Bag\n",
              "1    55997      Others  Colorbar Velvet Matte Temptation Lipstick 24MA\n",
              "2     2640       Shoes           Carlton London Men Brown Formal Shoes\n",
              "3    40565     Topwear                            W Women Maroon Kurta\n",
              "4    38932  Bottomwear               Gini and Jony Girls Pink Leggings"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set absolute path\n",
        "nb_path = os.path.abspath(\"a2.ipynb\")\n",
        "# nb_path = \"/content/drive/MyDrive/Colab Notebooks/cisc351-a2-fashion-classifier/images/a2.ipynb\"  # For Google Colab\n",
        "DIR_PATH = os.path.dirname(nb_path)\n",
        "\n",
        "# Load and preview the dataset\n",
        "train_df = pd.read_csv(os.path.join(DIR_PATH, 'train.csv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(DIR_PATH, 'test.csv'), sep='\\t')\n",
        "\n",
        "df = pd.concat([train_df, test_df], axis=0)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* FREQUENCY BY CATEGORY *\n",
            "label\n",
            "Topwear       15401\n",
            "Shoes          7344\n",
            "Others         6230\n",
            "Bags           3055\n",
            "Bottomwear     2693\n",
            "Watches        2542\n",
            "Innerwear      1808\n",
            "Jewellery      1080\n",
            "Eyewear        1073\n",
            "Fragrance      1012\n",
            "Sandal          963\n",
            "Wallets         933\n",
            "Makeup          307\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Make sure these are the only categories that appear in the dataset\n",
        "labels = set({\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"})\n",
        "unique_labels = df[\"label\"].unique()\n",
        "assert(labels == set(unique_labels))\n",
        "\n",
        "print(\"* FREQUENCY BY CATEGORY *\")\n",
        "print(df[\"label\"].value_counts(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS device.\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device.\")\n",
        "# Check if Mac GPU acceleration is available\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"MPS device not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"]\n",
        "label_dict = {label: index for index, label in enumerate(class_labels)}\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_file, images_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): path to csv file with `imageid` (file name) and `label`.\n",
        "            images_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(os.path.join(DIR_PATH, csv_file), sep='\\t')\n",
        "        self.df[\"label\"] = self.df[\"label\"].apply(lambda x: label_dict[x])  # convert the labels to numbers\n",
        "        self.images_dir = os.path.join(DIR_PATH, images_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = f\"{self.df.iloc[idx, 0]}.jpg\"\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert('RGB')  # some images are in grayscale\n",
        "        label = self.df.iloc[idx, 1]  # label is the second column\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_mean_std(loader: DataLoader):\n",
        "    \"\"\"\n",
        "    Compute the mean and standard deviation of the dataset for normalization.\n",
        "    Args:\n",
        "        loader (DataLoader): DataLoader with images to compute the mean and std of.\n",
        "    \"\"\"\n",
        "    mean = 0\n",
        "    std = 0\n",
        "    img_count = 0\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        img_count += images.size(0)\n",
        "    mean /= img_count\n",
        "    std /= img_count\n",
        "    return mean, std\n",
        "\n",
        "# Define the transformations for the initial loader to compute the mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Create a loader for computing the mean and std of the dataset, which we will use for normalization\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "mean, std = get_mean_std(train_loader)\n",
        "\n",
        "# Define the transformations for the actual train and test loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
        "test_data = FashionDataset(\"test.csv\", \"images\", transform=transform)\n",
        "\n",
        "# Create the loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a CNN model to classify the images\n",
        "class FashionClassifierCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionClassifierCNN, self).__init__()\n",
        "        # [(input - filter + 2*pad) / stride] + 1\n",
        "        # 72x72x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
        "        # 68x68x16\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 34x34x16\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        # 30x30x32\n",
        "        # apply max pooling again\n",
        "        # 15x15x32\n",
        "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 13)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = FashionClassifierCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1/100, batch 10/158]   loss: 1.883   time left: 15669s\n",
            "[epoch 1/100, batch 20/158]   loss: 1.137   time left: 15669s\n",
            "[epoch 1/100, batch 30/158]   loss: 0.816   time left: 15667s\n",
            "[epoch 1/100, batch 40/158]   loss: 0.668   time left: 15665s\n",
            "[epoch 1/100, batch 50/158]   loss: 0.576   time left: 15663s\n",
            "[epoch 1/100, batch 60/158]   loss: 0.538   time left: 15661s\n",
            "[epoch 1/100, batch 70/158]   loss: 0.524   time left: 15659s\n",
            "[epoch 1/100, batch 80/158]   loss: 0.503   time left: 15657s\n",
            "[epoch 1/100, batch 90/158]   loss: 0.441   time left: 15655s\n",
            "[epoch 1/100, batch 100/158]   loss: 0.413   time left: 15653s\n",
            "[epoch 1/100, batch 110/158]   loss: 0.422   time left: 15651s\n",
            "[epoch 1/100, batch 120/158]   loss: 0.415   time left: 15650s\n",
            "[epoch 1/100, batch 130/158]   loss: 0.396   time left: 15648s\n",
            "[epoch 1/100, batch 140/158]   loss: 0.363   time left: 15646s\n",
            "[epoch 1/100, batch 150/158]   loss: 0.335   time left: 15644s\n",
            "[epoch 2/100, batch 10/158]   loss: 0.337   time left: 15974s\n",
            "[epoch 2/100, batch 20/158]   loss: 0.327   time left: 15726s\n",
            "[epoch 2/100, batch 30/158]   loss: 0.303   time left: 15642s\n",
            "[epoch 2/100, batch 40/158]   loss: 0.294   time left: 15599s\n",
            "[epoch 2/100, batch 50/158]   loss: 0.284   time left: 15573s\n",
            "[epoch 2/100, batch 60/158]   loss: 0.301   time left: 15554s\n",
            "[epoch 2/100, batch 70/158]   loss: 0.285   time left: 15541s\n",
            "[epoch 2/100, batch 80/158]   loss: 0.276   time left: 15530s\n",
            "[epoch 2/100, batch 90/158]   loss: 0.277   time left: 15521s\n",
            "[epoch 2/100, batch 100/158]   loss: 0.276   time left: 15514s\n",
            "[epoch 2/100, batch 110/158]   loss: 0.246   time left: 15507s\n",
            "[epoch 2/100, batch 120/158]   loss: 0.261   time left: 15502s\n",
            "[epoch 2/100, batch 130/158]   loss: 0.252   time left: 15496s\n",
            "[epoch 2/100, batch 140/158]   loss: 0.246   time left: 15492s\n",
            "[epoch 2/100, batch 150/158]   loss: 0.239   time left: 15488s\n",
            "[epoch 3/100, batch 10/158]   loss: 0.237   time left: 16263s\n",
            "[epoch 3/100, batch 20/158]   loss: 0.198   time left: 15776s\n",
            "[epoch 3/100, batch 30/158]   loss: 0.213   time left: 15612s\n",
            "[epoch 3/100, batch 40/158]   loss: 0.205   time left: 15529s\n",
            "[epoch 3/100, batch 50/158]   loss: 0.207   time left: 15479s\n",
            "[epoch 3/100, batch 60/158]   loss: 0.206   time left: 15445s\n",
            "[epoch 3/100, batch 70/158]   loss: 0.197   time left: 15420s\n",
            "[epoch 3/100, batch 80/158]   loss: 0.231   time left: 15401s\n",
            "[epoch 3/100, batch 90/158]   loss: 0.202   time left: 15386s\n",
            "[epoch 3/100, batch 100/158]   loss: 0.198   time left: 15373s\n",
            "[epoch 3/100, batch 110/158]   loss: 0.198   time left: 15363s\n",
            "[epoch 3/100, batch 120/158]   loss: 0.220   time left: 15353s\n",
            "[epoch 3/100, batch 130/158]   loss: 0.196   time left: 15345s\n",
            "[epoch 3/100, batch 140/158]   loss: 0.212   time left: 15338s\n",
            "[epoch 3/100, batch 150/158]   loss: 0.212   time left: 15331s\n",
            "[epoch 4/100, batch 10/158]   loss: 0.173   time left: 16547s\n",
            "[epoch 4/100, batch 20/158]   loss: 0.166   time left: 15823s\n",
            "[epoch 4/100, batch 30/158]   loss: 0.183   time left: 15581s\n",
            "[epoch 4/100, batch 40/158]   loss: 0.152   time left: 15459s\n",
            "[epoch 4/100, batch 50/158]   loss: 0.167   time left: 15385s\n",
            "[epoch 4/100, batch 60/158]   loss: 0.171   time left: 15335s\n",
            "[epoch 4/100, batch 70/158]   loss: 0.185   time left: 15299s\n",
            "[epoch 4/100, batch 80/158]   loss: 0.157   time left: 15272s\n",
            "[epoch 4/100, batch 90/158]   loss: 0.158   time left: 15250s\n",
            "[epoch 4/100, batch 100/158]   loss: 0.172   time left: 15232s\n",
            "[epoch 4/100, batch 110/158]   loss: 0.191   time left: 15217s\n",
            "[epoch 4/100, batch 120/158]   loss: 0.158   time left: 15205s\n",
            "[epoch 4/100, batch 130/158]   loss: 0.169   time left: 15193s\n",
            "[epoch 4/100, batch 140/158]   loss: 0.173   time left: 15184s\n",
            "[epoch 4/100, batch 150/158]   loss: 0.161   time left: 15175s\n",
            "[epoch 5/100, batch 10/158]   loss: 0.154   time left: 16812s\n",
            "[epoch 5/100, batch 20/158]   loss: 0.137   time left: 15863s\n",
            "[epoch 5/100, batch 30/158]   loss: 0.148   time left: 15545s\n",
            "[epoch 5/100, batch 40/158]   loss: 0.143   time left: 15386s\n",
            "[epoch 5/100, batch 50/158]   loss: 0.135   time left: 15289s\n",
            "[epoch 5/100, batch 60/158]   loss: 0.129   time left: 15224s\n",
            "[epoch 5/100, batch 70/158]   loss: 0.144   time left: 15177s\n",
            "[epoch 5/100, batch 80/158]   loss: 0.136   time left: 15142s\n",
            "[epoch 5/100, batch 90/158]   loss: 0.130   time left: 15114s\n",
            "[epoch 5/100, batch 100/158]   loss: 0.128   time left: 15091s\n",
            "[epoch 5/100, batch 110/158]   loss: 0.138   time left: 15072s\n",
            "[epoch 5/100, batch 120/158]   loss: 0.128   time left: 15056s\n",
            "[epoch 5/100, batch 130/158]   loss: 0.124   time left: 15042s\n",
            "[epoch 5/100, batch 140/158]   loss: 0.132   time left: 15030s\n",
            "[epoch 5/100, batch 150/158]   loss: 0.138   time left: 15019s\n",
            "[epoch 6/100, batch 10/158]   loss: 0.106   time left: 17078s\n",
            "[epoch 6/100, batch 20/158]   loss: 0.121   time left: 15902s\n",
            "[epoch 6/100, batch 30/158]   loss: 0.107   time left: 15510s\n",
            "[epoch 6/100, batch 40/158]   loss: 0.115   time left: 15312s\n",
            "[epoch 6/100, batch 50/158]   loss: 0.115   time left: 15193s\n",
            "[epoch 6/100, batch 60/158]   loss: 0.101   time left: 15113s\n",
            "[epoch 6/100, batch 70/158]   loss: 0.119   time left: 15055s\n",
            "[epoch 6/100, batch 80/158]   loss: 0.113   time left: 15012s\n",
            "[epoch 6/100, batch 90/158]   loss: 0.117   time left: 14978s\n",
            "[epoch 6/100, batch 100/158]   loss: 0.113   time left: 14950s\n",
            "[epoch 6/100, batch 110/158]   loss: 0.105   time left: 14927s\n",
            "[epoch 6/100, batch 120/158]   loss: 0.117   time left: 14907s\n",
            "[epoch 6/100, batch 130/158]   loss: 0.127   time left: 14890s\n",
            "[epoch 6/100, batch 140/158]   loss: 0.109   time left: 14875s\n",
            "[epoch 6/100, batch 150/158]   loss: 0.108   time left: 14862s\n",
            "[epoch 7/100, batch 10/158]   loss: 0.079   time left: 17343s\n",
            "[epoch 7/100, batch 20/158]   loss: 0.081   time left: 15942s\n",
            "[epoch 7/100, batch 30/158]   loss: 0.098   time left: 15474s\n",
            "[epoch 7/100, batch 40/158]   loss: 0.083   time left: 15239s\n",
            "[epoch 7/100, batch 50/158]   loss: 0.098   time left: 15097s\n",
            "[epoch 7/100, batch 60/158]   loss: 0.070   time left: 15002s\n",
            "[epoch 7/100, batch 70/158]   loss: 0.079   time left: 14934s\n",
            "[epoch 7/100, batch 80/158]   loss: 0.108   time left: 14882s\n",
            "[epoch 7/100, batch 90/158]   loss: 0.109   time left: 14841s\n",
            "[epoch 7/100, batch 100/158]   loss: 0.102   time left: 14808s\n",
            "[epoch 7/100, batch 110/158]   loss: 0.120   time left: 14781s\n",
            "[epoch 7/100, batch 120/158]   loss: 0.095   time left: 14758s\n",
            "[epoch 7/100, batch 130/158]   loss: 0.103   time left: 14739s\n",
            "[epoch 7/100, batch 140/158]   loss: 0.070   time left: 14721s\n",
            "[epoch 7/100, batch 150/158]   loss: 0.094   time left: 14706s\n",
            "[epoch 8/100, batch 10/158]   loss: 0.068   time left: 17613s\n",
            "[epoch 8/100, batch 20/158]   loss: 0.076   time left: 15984s\n",
            "[epoch 8/100, batch 30/158]   loss: 0.073   time left: 15439s\n",
            "[epoch 8/100, batch 40/158]   loss: 0.066   time left: 15166s\n",
            "[epoch 8/100, batch 50/158]   loss: 0.091   time left: 15002s\n",
            "[epoch 8/100, batch 60/158]   loss: 0.083   time left: 14891s\n",
            "[epoch 8/100, batch 70/158]   loss: 0.071   time left: 14812s\n",
            "[epoch 8/100, batch 80/158]   loss: 0.082   time left: 14752s\n",
            "[epoch 8/100, batch 90/158]   loss: 0.072   time left: 14705s\n",
            "[epoch 8/100, batch 100/158]   loss: 0.072   time left: 14667s\n",
            "[epoch 8/100, batch 110/158]   loss: 0.070   time left: 14636s\n",
            "[epoch 8/100, batch 120/158]   loss: 0.072   time left: 14610s\n",
            "[epoch 8/100, batch 130/158]   loss: 0.067   time left: 14587s\n",
            "[epoch 8/100, batch 140/158]   loss: 0.091   time left: 14567s\n",
            "[epoch 8/100, batch 150/158]   loss: 0.084   time left: 14550s\n",
            "[epoch 9/100, batch 10/158]   loss: 0.067   time left: 17876s\n",
            "[epoch 9/100, batch 20/158]   loss: 0.054   time left: 16022s\n",
            "[epoch 9/100, batch 30/158]   loss: 0.045   time left: 15403s\n",
            "[epoch 9/100, batch 40/158]   loss: 0.054   time left: 15092s\n",
            "[epoch 9/100, batch 50/158]   loss: 0.048   time left: 14905s\n",
            "[epoch 9/100, batch 60/158]   loss: 0.065   time left: 14780s\n",
            "[epoch 9/100, batch 70/158]   loss: 0.064   time left: 14690s\n",
            "[epoch 9/100, batch 80/158]   loss: 0.058   time left: 14622s\n",
            "[epoch 9/100, batch 90/158]   loss: 0.071   time left: 14569s\n",
            "[epoch 9/100, batch 100/158]   loss: 0.060   time left: 14526s\n",
            "[epoch 9/100, batch 110/158]   loss: 0.059   time left: 14491s\n",
            "[epoch 9/100, batch 120/158]   loss: 0.055   time left: 14461s\n",
            "[epoch 9/100, batch 130/158]   loss: 0.078   time left: 14435s\n",
            "[epoch 9/100, batch 140/158]   loss: 0.076   time left: 14413s\n",
            "[epoch 9/100, batch 150/158]   loss: 0.066   time left: 14394s\n",
            "[epoch 10/100, batch 10/158]   loss: 0.050   time left: 18138s\n",
            "[epoch 10/100, batch 20/158]   loss: 0.037   time left: 16060s\n",
            "[epoch 10/100, batch 30/158]   loss: 0.030   time left: 15366s\n",
            "[epoch 10/100, batch 40/158]   loss: 0.045   time left: 15018s\n",
            "[epoch 10/100, batch 50/158]   loss: 0.037   time left: 14809s\n",
            "[epoch 10/100, batch 60/158]   loss: 0.033   time left: 14668s\n",
            "[epoch 10/100, batch 70/158]   loss: 0.043   time left: 14568s\n",
            "[epoch 10/100, batch 80/158]   loss: 0.056   time left: 14492s\n",
            "[epoch 10/100, batch 90/158]   loss: 0.039   time left: 14432s\n",
            "[epoch 10/100, batch 100/158]   loss: 0.054   time left: 14385s\n",
            "[epoch 10/100, batch 110/158]   loss: 0.047   time left: 14345s\n",
            "[epoch 10/100, batch 120/158]   loss: 0.056   time left: 14312s\n",
            "[epoch 10/100, batch 130/158]   loss: 0.068   time left: 14283s\n",
            "[epoch 10/100, batch 140/158]   loss: 0.058   time left: 14259s\n",
            "[epoch 10/100, batch 150/158]   loss: 0.043   time left: 14237s\n",
            "[epoch 11/100, batch 10/158]   loss: 0.046   time left: 18399s\n",
            "[epoch 11/100, batch 20/158]   loss: 0.036   time left: 16097s\n",
            "[epoch 11/100, batch 30/158]   loss: 0.025   time left: 15328s\n",
            "[epoch 11/100, batch 40/158]   loss: 0.033   time left: 14943s\n",
            "[epoch 11/100, batch 50/158]   loss: 0.041   time left: 14712s\n",
            "[epoch 11/100, batch 60/158]   loss: 0.040   time left: 14557s\n",
            "[epoch 11/100, batch 70/158]   loss: 0.042   time left: 14446s\n",
            "[epoch 11/100, batch 80/158]   loss: 0.040   time left: 14362s\n",
            "[epoch 11/100, batch 90/158]   loss: 0.046   time left: 14296s\n",
            "[epoch 11/100, batch 100/158]   loss: 0.040   time left: 14243s\n",
            "[epoch 11/100, batch 110/158]   loss: 0.045   time left: 14200s\n",
            "[epoch 11/100, batch 120/158]   loss: 0.045   time left: 14163s\n",
            "[epoch 11/100, batch 130/158]   loss: 0.041   time left: 14132s\n",
            "[epoch 11/100, batch 140/158]   loss: 0.048   time left: 14105s\n",
            "[epoch 11/100, batch 150/158]   loss: 0.042   time left: 14081s\n",
            "[epoch 12/100, batch 10/158]   loss: 0.025   time left: 18657s\n",
            "[epoch 12/100, batch 20/158]   loss: 0.034   time left: 16133s\n",
            "[epoch 12/100, batch 30/158]   loss: 0.024   time left: 15291s\n",
            "[epoch 12/100, batch 40/158]   loss: 0.033   time left: 14869s\n",
            "[epoch 12/100, batch 50/158]   loss: 0.031   time left: 14615s\n",
            "[epoch 12/100, batch 60/158]   loss: 0.040   time left: 14445s\n",
            "[epoch 12/100, batch 70/158]   loss: 0.035   time left: 14323s\n",
            "[epoch 12/100, batch 80/158]   loss: 0.040   time left: 14231s\n",
            "[epoch 12/100, batch 90/158]   loss: 0.031   time left: 14159s\n",
            "[epoch 12/100, batch 100/158]   loss: 0.030   time left: 14102s\n",
            "[epoch 12/100, batch 110/158]   loss: 0.034   time left: 14054s\n",
            "[epoch 12/100, batch 120/158]   loss: 0.035   time left: 14014s\n",
            "[epoch 12/100, batch 130/158]   loss: 0.036   time left: 13980s\n",
            "[epoch 12/100, batch 140/158]   loss: 0.044   time left: 13950s\n",
            "[epoch 12/100, batch 150/158]   loss: 0.053   time left: 13925s\n",
            "[epoch 13/100, batch 10/158]   loss: 0.037   time left: 18913s\n",
            "[epoch 13/100, batch 20/158]   loss: 0.033   time left: 16168s\n",
            "[epoch 13/100, batch 30/158]   loss: 0.029   time left: 15252s\n",
            "[epoch 13/100, batch 40/158]   loss: 0.030   time left: 14793s\n",
            "[epoch 13/100, batch 50/158]   loss: 0.051   time left: 14517s\n",
            "[epoch 13/100, batch 60/158]   loss: 0.029   time left: 14333s\n",
            "[epoch 13/100, batch 70/158]   loss: 0.031   time left: 14200s\n",
            "[epoch 13/100, batch 80/158]   loss: 0.027   time left: 14101s\n",
            "Early stopping!\n",
            "Finished training. Best model (loss: 0.024 saved to model.pt\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model = FashionClassifierCNN().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model.pt\")))\n",
        "\n",
        "epochs = 100\n",
        "no_improvement_streak = 0\n",
        "patience = 20\n",
        "best_loss = float('inf')\n",
        "stop = False\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    # For each batch\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()  # Reset the gradients\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            avg_loss = total_loss / 10\n",
        "            time_elapsed = time.time() - time_start\n",
        "            time_left = (time_elapsed * (len(train_loader) - i) / (i + 1)\n",
        "                         + (epochs - epoch - 1) * len(train_loader))\n",
        "            print(f\"[epoch {epoch + 1}/{epochs}, batch {i + 1}/{len(train_loader)}]   loss: {avg_loss:.3f}   time left: {time_left:.0f}s\")\n",
        "            total_loss = 0.0\n",
        "\n",
        "            # Early stopping mechanism\n",
        "            if avg_loss < best_loss:\n",
        "                # save the model if it's the best so far\n",
        "                torch.save(model.state_dict(), os.path.join(DIR_PATH, \"model.pt\"))\n",
        "                best_loss = avg_loss\n",
        "                no_improvement_streak = 0\n",
        "            else:\n",
        "                no_improvement_streak += 1\n",
        "                \n",
        "        if no_improvement_streak == patience:\n",
        "            stop = True\n",
        "            break\n",
        "    if stop:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "\n",
        "print(f\"Finished training. Best model (loss: {best_loss:.3f} saved to model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 4000 test images: 94.9%\n"
          ]
        }
      ],
      "source": [
        "# If you want to load the model from the file\n",
        "# model = FashionClassifierCNN().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(DIR_PATH, \"model.pt\")))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # For each batch\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
