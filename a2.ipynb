{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set absolute path\n",
    "nb_path = os.path.abspath(\"a2.ipynb\")\n",
    "DIR_PATH = os.path.dirname(nb_path)\n",
    "\n",
    "# Load and preview the dataset\n",
    "train_df = pd.read_csv('train.csv', sep='\\t')\n",
    "test_df = pd.read_csv('test.csv', sep='\\t')\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* FREQUENCY BY CATEGORY *\n",
      "label\n",
      "Topwear       15401\n",
      "Shoes          7344\n",
      "Others         6230\n",
      "Bags           3055\n",
      "Bottomwear     2693\n",
      "Watches        2542\n",
      "Innerwear      1808\n",
      "Jewellery      1080\n",
      "Eyewear        1073\n",
      "Fragrance      1012\n",
      "Sandal          963\n",
      "Wallets         933\n",
      "Makeup          307\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure these are the only categories that appear in the dataset\n",
    "labels = set({\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"})\n",
    "unique_labels = df[\"label\"].unique()\n",
    "assert(labels == set(unique_labels))\n",
    "\n",
    "print(\"* FREQUENCY BY CATEGORY *\")\n",
    "print(df[\"label\"].value_counts(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"]\n",
    "label_dict = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): path to csv file with `imageid` (file name) and `label`.\n",
    "            images_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(os.path.join(DIR_PATH, csv_file), sep='\\t')\n",
    "        self.df[\"label\"] = self.df[\"label\"].apply(lambda x: label_dict[x])  # convert the labels to numbers\n",
    "        self.images_dir = os.path.join(DIR_PATH, images_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = f\"{self.df.iloc[idx, 0]}.jpg\"\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')  # some images are in grayscale\n",
    "        label = self.df.iloc[idx, 1]  # label is the second column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mean_std(loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of the dataset for normalization.\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader with images to compute the mean and std of.\n",
    "    \"\"\"\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    img_count = 0\n",
    "    for images, _ in loader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        img_count += images.size(0)\n",
    "    mean /= img_count\n",
    "    std /= img_count\n",
    "    return mean, std\n",
    "\n",
    "# Define the transformations for the initial loader to compute the mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Create a loader for computing the mean and std of the dataset, which we will use for normalization\n",
    "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "mean, std = get_mean_std(train_loader)\n",
    "\n",
    "# Define the transformations for the actual train and test loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Create the datasets\n",
    "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
    "test_data = FashionDataset(\"test.csv\", \"images\", transform=transform)\n",
    "\n",
    "# Create the loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model to classify the images\n",
    "class FashionClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionClassifierCNN, self).__init__()\n",
    "        # [(input - filter + 2*pad) / stride] + 1\n",
    "        # 72x72x3\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
    "        # 68x68x16\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # 34x34x16\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        # 30x30x32\n",
    "        # apply max pooling again\n",
    "        # 15x15x32\n",
    "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = FashionClassifierCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/10, batch 10/158] loss: 0.517\n",
      "[epoch 1/10, batch 20/158] loss: 0.515\n",
      "[epoch 1/10, batch 30/158] loss: 0.466\n",
      "[epoch 1/10, batch 40/158] loss: 0.427\n",
      "[epoch 1/10, batch 50/158] loss: 0.418\n",
      "[epoch 1/10, batch 60/158] loss: 0.411\n",
      "[epoch 1/10, batch 70/158] loss: 0.394\n",
      "[epoch 1/10, batch 80/158] loss: 0.379\n",
      "[epoch 1/10, batch 90/158] loss: 0.383\n",
      "[epoch 1/10, batch 100/158] loss: 0.375\n",
      "[epoch 1/10, batch 110/158] loss: 0.330\n",
      "[epoch 1/10, batch 120/158] loss: 0.325\n",
      "[epoch 1/10, batch 130/158] loss: 0.344\n",
      "[epoch 1/10, batch 140/158] loss: 0.329\n",
      "[epoch 1/10, batch 150/158] loss: 0.310\n",
      "[epoch 2/10, batch 10/158] loss: 0.294\n",
      "[epoch 2/10, batch 20/158] loss: 0.285\n",
      "[epoch 2/10, batch 30/158] loss: 0.318\n",
      "[epoch 2/10, batch 40/158] loss: 0.261\n",
      "[epoch 2/10, batch 50/158] loss: 0.267\n",
      "[epoch 2/10, batch 60/158] loss: 0.277\n",
      "[epoch 2/10, batch 70/158] loss: 0.257\n",
      "[epoch 2/10, batch 80/158] loss: 0.294\n",
      "[epoch 2/10, batch 90/158] loss: 0.255\n",
      "[epoch 2/10, batch 100/158] loss: 0.265\n",
      "[epoch 2/10, batch 110/158] loss: 0.251\n",
      "[epoch 2/10, batch 120/158] loss: 0.260\n",
      "[epoch 2/10, batch 130/158] loss: 0.254\n",
      "[epoch 2/10, batch 140/158] loss: 0.225\n",
      "[epoch 2/10, batch 150/158] loss: 0.237\n",
      "[epoch 3/10, batch 10/158] loss: 0.211\n",
      "[epoch 3/10, batch 20/158] loss: 0.204\n",
      "[epoch 3/10, batch 30/158] loss: 0.215\n",
      "[epoch 3/10, batch 40/158] loss: 0.231\n",
      "[epoch 3/10, batch 50/158] loss: 0.211\n",
      "[epoch 3/10, batch 60/158] loss: 0.198\n",
      "[epoch 3/10, batch 70/158] loss: 0.182\n",
      "[epoch 3/10, batch 80/158] loss: 0.202\n",
      "[epoch 3/10, batch 90/158] loss: 0.215\n",
      "[epoch 3/10, batch 100/158] loss: 0.210\n",
      "[epoch 3/10, batch 110/158] loss: 0.215\n",
      "[epoch 3/10, batch 120/158] loss: 0.207\n",
      "[epoch 3/10, batch 130/158] loss: 0.191\n",
      "[epoch 3/10, batch 140/158] loss: 0.199\n",
      "[epoch 3/10, batch 150/158] loss: 0.189\n",
      "[epoch 4/10, batch 10/158] loss: 0.165\n",
      "[epoch 4/10, batch 20/158] loss: 0.163\n",
      "[epoch 4/10, batch 30/158] loss: 0.175\n",
      "[epoch 4/10, batch 40/158] loss: 0.189\n",
      "[epoch 4/10, batch 50/158] loss: 0.171\n",
      "[epoch 4/10, batch 60/158] loss: 0.175\n",
      "[epoch 4/10, batch 70/158] loss: 0.167\n",
      "[epoch 4/10, batch 80/158] loss: 0.191\n",
      "[epoch 4/10, batch 90/158] loss: 0.167\n",
      "[epoch 4/10, batch 100/158] loss: 0.177\n",
      "[epoch 4/10, batch 110/158] loss: 0.186\n",
      "[epoch 4/10, batch 120/158] loss: 0.169\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "no_improvement_streak = 0\n",
    "patience = 10\n",
    "best_loss = float('inf')\n",
    "stop = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    # For each batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # Reset the gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            avg_loss = total_loss / 10\n",
    "            print(f\"[epoch {epoch + 1}/{epochs}, batch {i + 1}/{len(train_loader)}] loss: {avg_loss:.3f}\")\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Early stopping mechanism\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                no_improvement_streak = 0\n",
    "            else:\n",
    "                no_improvement_streak += 1\n",
    "                \n",
    "        if no_improvement_streak == patience:\n",
    "            stop = True\n",
    "            break\n",
    "    if stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 4000 test images: 93.425%\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # For each batch\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(DIR_PATH, \"model.pth\"))\n",
    "print(\"Model saved to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
