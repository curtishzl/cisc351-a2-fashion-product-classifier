{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>label</th>\n",
       "      <th>productname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2653</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Murcia Women Leather Office Grey Bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55997</td>\n",
       "      <td>Others</td>\n",
       "      <td>Colorbar Velvet Matte Temptation Lipstick 24MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2640</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Carlton London Men Brown Formal Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40565</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>W Women Maroon Kurta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38932</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Gini and Jony Girls Pink Leggings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imageid       label                                     productname\n",
       "0     2653        Bags            Murcia Women Leather Office Grey Bag\n",
       "1    55997      Others  Colorbar Velvet Matte Temptation Lipstick 24MA\n",
       "2     2640       Shoes           Carlton London Men Brown Formal Shoes\n",
       "3    40565     Topwear                            W Women Maroon Kurta\n",
       "4    38932  Bottomwear               Gini and Jony Girls Pink Leggings"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set absolute path\n",
    "nb_path = os.path.abspath(\"a2.ipynb\")\n",
    "# nb_path = \"/content/drive/MyDrive/Colab Notebooks/cisc351-a2-fashion-classifier/images/a2.ipynb\"  # For Google Colab\n",
    "DIR_PATH = os.path.dirname(nb_path)\n",
    "\n",
    "# Load and preview the dataset\n",
    "train_df = pd.read_csv(os.path.join(DIR_PATH, 'train.csv'), sep='\\t')\n",
    "test_df = pd.read_csv(os.path.join(DIR_PATH, 'test.csv'), sep='\\t')\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* FREQUENCY BY CATEGORY *\n",
      "label\n",
      "Topwear       15401\n",
      "Shoes          7344\n",
      "Others         6230\n",
      "Bags           3055\n",
      "Bottomwear     2693\n",
      "Watches        2542\n",
      "Innerwear      1808\n",
      "Jewellery      1080\n",
      "Eyewear        1073\n",
      "Fragrance      1012\n",
      "Sandal          963\n",
      "Wallets         933\n",
      "Makeup          307\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure these are the only categories that appear in the dataset\n",
    "labels = set({\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"})\n",
    "unique_labels = df[\"label\"].unique()\n",
    "assert(labels == set(unique_labels))\n",
    "\n",
    "print(\"* FREQUENCY BY CATEGORY *\")\n",
    "print(df[\"label\"].value_counts(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Others\"]\n",
    "label_dict = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): path to csv file with `imageid` (file name) and `label`.\n",
    "            images_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(os.path.join(DIR_PATH, csv_file), sep='\\t')\n",
    "        self.df[\"label\"] = self.df[\"label\"].apply(lambda x: label_dict[x])  # convert the labels to numbers\n",
    "        self.images_dir = os.path.join(DIR_PATH, images_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = f\"{self.df.iloc[idx, 0]}.jpg\"\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')  # some images are in grayscale\n",
    "        label = self.df.iloc[idx, 1]  # label is the second column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/Users/curtishzl/Documents/GitHub/cisc351/cisc351-a2/images/13971.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m train_data \u001b[38;5;241m=\u001b[39m FashionDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     29\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m get_mean_std(train_loader)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the transformations for the actual train and test loaders\u001b[39;00m\n\u001b[1;32m     33\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     34\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m72\u001b[39m, \u001b[38;5;241m72\u001b[39m)),\n\u001b[1;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     36\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean, std)\n\u001b[1;32m     37\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mget_mean_std\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m img_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, _ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36mFashionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, img_name)\n\u001b[0;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[1;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# some images are in grayscale\u001b[39;00m\n\u001b[1;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# label is the second column\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3280\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3275\u001b[0m         _decompression_bomb_check(im.size)\n\u001b[1;32m   3276\u001b[0m         return im\n\u001b[1;32m   3277\u001b[0m except (SyntaxError, IndexError, TypeError, struct.error):\n\u001b[1;32m   3278\u001b[0m     # Leave disabled by default, spams the logs with image\n\u001b[1;32m   3279\u001b[0m     # opening failures that are entirely expected.\n\u001b[0;32m-> 3280\u001b[0m     # logger.debug(\"\", exc_info=True)\n\u001b[1;32m   3281\u001b[0m     continue\n\u001b[1;32m   3282\u001b[0m except BaseException:\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/Users/curtishzl/Documents/GitHub/cisc351/cisc351-a2/images/13971.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_mean_std(loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of the dataset for normalization.\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader with images to compute the mean and std of.\n",
    "    \"\"\"\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    img_count = 0\n",
    "    for images, _ in loader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        img_count += images.size(0)\n",
    "    mean /= img_count\n",
    "    std /= img_count\n",
    "    return mean, std\n",
    "\n",
    "# Define the transformations for the initial loader to compute the mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Create a loader for computing the mean and std of the dataset, which we will use for normalization\n",
    "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "mean, std = get_mean_std(train_loader)\n",
    "\n",
    "# Define the transformations for the actual train and test loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Create the datasets\n",
    "train_data = FashionDataset(\"train.csv\", \"images\", transform=transform)\n",
    "test_data = FashionDataset(\"test.csv\", \"images\", transform=transform)\n",
    "\n",
    "# Create the loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model to classify the images\n",
    "class FashionClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionClassifierCNN, self).__init__()\n",
    "        # [(input - filter + 2*pad) / stride] + 1\n",
    "        # 72x72x3\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)  # Input channels, output channels, kernel size\n",
    "        # 68x68x16\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # 34x34x16\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        # 30x30x32\n",
    "        # apply max pooling again\n",
    "        # 15x15x32\n",
    "        self.fc1 = nn.Linear(in_features=15*15*32, out_features=120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = FashionClassifierCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/1, batch 10/158] loss: 1.901\n",
      "[epoch 1/1, batch 20/158] loss: 1.119\n",
      "[epoch 1/1, batch 30/158] loss: 0.856\n",
      "[epoch 1/1, batch 40/158] loss: 0.661\n",
      "[epoch 1/1, batch 50/158] loss: 0.606\n",
      "[epoch 1/1, batch 60/158] loss: 0.510\n",
      "[epoch 1/1, batch 70/158] loss: 0.488\n",
      "[epoch 1/1, batch 80/158] loss: 0.436\n",
      "[epoch 1/1, batch 90/158] loss: 0.418\n",
      "[epoch 1/1, batch 100/158] loss: 0.412\n",
      "[epoch 1/1, batch 110/158] loss: 0.366\n",
      "[epoch 1/1, batch 120/158] loss: 0.367\n",
      "[epoch 1/1, batch 130/158] loss: 0.347\n",
      "[epoch 1/1, batch 140/158] loss: 0.343\n",
      "[epoch 1/1, batch 150/158] loss: 0.324\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "no_improvement_streak = 0\n",
    "patience = 10\n",
    "best_loss = float('inf')\n",
    "stop = False\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    # For each batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # Reset the gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            avg_loss = total_loss / 10\n",
    "            time_elapsed = time.time() - time_start\n",
    "            time_left = (time_elapsed * (len(train_loader) - i) / (i + 1)\n",
    "                         + (epochs - epoch - 1) * len(train_loader))\n",
    "            print(f\"[epoch {epoch + 1}/{epochs}, batch {i + 1}/{len(train_loader)}]   loss: {avg_loss:.3f}   time left: {time_left:.0f}s\")\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Early stopping mechanism\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                no_improvement_streak = 0\n",
    "            else:\n",
    "                no_improvement_streak += 1\n",
    "                \n",
    "        if no_improvement_streak == patience:\n",
    "            stop = True\n",
    "            break\n",
    "    if stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 4000 test images: 89.675%\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # For each batch\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(DIR_PATH, \"model.pth\"))\n",
    "print(\"Model saved to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
